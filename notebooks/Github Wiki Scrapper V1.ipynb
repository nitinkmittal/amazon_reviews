{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from numpy import random\n",
    "from dateutil.parser import parse\n",
    "from re import search, escape, sub, findall\n",
    "import re\n",
    "import numpy as np\n",
    "from os import mkdir, getcwd, path\n",
    "from time import time\n",
    "from glob import glob\n",
    "from IPython.display import Audio\n",
    "from datetime import datetime, timedelta\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening google chrome\n",
    "driver = webdriver.Edge(r'C:\\Users\\user\\Downloads\\Softwares\\edgedriver_win64\\msedgedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = 300.0\n",
    "# f2 = 300.0\n",
    "# rate = 8000.0\n",
    "# L = 3\n",
    "# times = np.linspace(0,L,int(rate*L))\n",
    "# signal = np.sin(2*np.pi*f1*times) + np.sin(2*np.pi*f2*times)\n",
    "# i=0\n",
    "# while i < len(signal):\n",
    "#     signal[i:i+1000] = np.array([0]*1000)\n",
    "#     i+=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(name):\n",
    "    print(\"Creating folder: %s\"%path.basename(name))\n",
    "    try:  \n",
    "        mkdir(name)\n",
    "    except OSError as error:  \n",
    "        print(error)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: Wink-Wikis\n"
     ]
    }
   ],
   "source": [
    "MIN_SLEEP_TIME = 10\n",
    "MAX_SLEEP_TIME = 15\n",
    "\n",
    "PLATFORM_NAME = \"Wink-Wikis\"\n",
    "start_date = '2012-01-01'\n",
    "end_date = '2020-07-01'\n",
    "DATA_PATH  = getcwd() + '\\\\' + PLATFORM_NAME\n",
    "create_dir(DATA_PATH)\n",
    "DATA_PATH += \"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sleep():\n",
    "    sleep_time = random.choice([i for i in range(MIN_SLEEP_TIME,MAX_SLEEP_TIME)])\n",
    "    print(\"- Sleeping for %d seconds -\"%sleep_time)\n",
    "    sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the page\n",
    "def load_page_url(driver, url):\n",
    "    print(\"- Loading page using driver -\")\n",
    "    try:\n",
    "        driver.get(url) # loading page\n",
    "    except:\n",
    "        print(\"- Loading page failed -\")\n",
    "    random_sleep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to get the page source code\n",
    "def get_page_source(driver):\n",
    "    print(\"- Getting Page Source -\")\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\") # using Beautiful Soup Module to parse and process the source \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert string date to date format\n",
    "def string_to_date(date):\n",
    "    return datetime.strptime(date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = sub('\\n','', text) # to remove newlines\n",
    "    text = sub(r'[ ]+',' ', text) # to remove extra space\n",
    "    text = text.strip() # to remove space at start and end\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_n_issues(issuses):\n",
    "    issuses = sub(',','', issuses)\n",
    "    return int(findall(r'[\\d]+',issuses)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load issues between specified date\n",
    "def load_github_issues(start_date, end_date):\n",
    "    \n",
    "    URL = \"https://github.com/search?p=1&q=%28%22wink+api%22+OR+%22wink+developer%22+OR+%22wink+hub%22%29+updated%3A\" + start_date + \"..\" + end_date + \"&type=Wikis\"\n",
    "    load_page_url(driver, URL)\n",
    "    pg_source = get_page_source(driver)\n",
    "    n_issues = pg_source.find(name='div', attrs={'class':'d-flex flex-column flex-md-row flex-justify-between border-bottom pb-3 position-relative'})\n",
    "    n_issues = identify_n_issues(clean_text(n_issues.find('h3').text))\n",
    "    issues_per_page = 10\n",
    "    pages_to_load = int(n_issues/issues_per_page)\n",
    "    if n_issues/issues_per_page > 0 :\n",
    "        pages_to_load+=1\n",
    "        \n",
    "    return n_issues, pages_to_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to decrease end date to lessen the number of results\n",
    "def decrease_end_date(start_date, end_date, smart_decrease = False, cur_result_cnt=0, max_result_cnt = 1000):\n",
    "    \n",
    "    print(\"- Decreasing end date (smart decrease: %s) -\"%str(smart_decrease))\n",
    "    start_date = string_to_date(start_date)\n",
    "    end_date = string_to_date(end_date)\n",
    "    \n",
    "    # getting current difference\n",
    "    dif_days = end_date - start_date\n",
    "    dif_days = dif_days.days \n",
    "    \n",
    "    print(\"- Current difference between %s and %s: %d days -\"%(str(start_date), str(end_date), dif_days))\n",
    "    if (smart_decrease) and (cur_result_cnt > 0):\n",
    "        end_date = start_date + timedelta(days= int(max_result_cnt/(cur_result_cnt/dif_days)))\n",
    "    else:\n",
    "        end_date = start_date + timedelta(days=int(dif_days/2))\n",
    "    \n",
    "    # getting new difference\n",
    "    dif_days = end_date - start_date\n",
    "    dif_days = dif_days.days \n",
    "    print(\"- Updated difference between %s and %s: %d days -\"%(str(start_date), str(end_date), dif_days))\n",
    "        \n",
    "    return str(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract issues from a page\n",
    "def extract_page_issues(page_source):\n",
    "    issues = page_source.find_all(name='div', attrs={'class':'hx_hit-wiki py-4 border-top'})\n",
    "    page_data = []\n",
    "    for issue in issues:\n",
    "        user_name = issue.find(name='a', attrs={'class':'muted-link text-small text-bold'}).text\n",
    "        open_date = issue.find(name='relative-time')['title']\n",
    "        wiki_id = issue.find(name='div', attrs={'class':'f4 text-normal'}).find('a')['data-hydro-click-hmac']\n",
    "\n",
    "        page_data.append([user_name, open_date, wiki_id])\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save issues from a page\n",
    "def save_page_issues(page_issues, start_date, end_date, page_no, type_='txt'):\n",
    "    print('- Writing %d issues from page_no: %d between dates %s and %s -'%(len(page_issues),page_no,start_date, end_date))\n",
    "    # write data in a file. \n",
    "    if type_ == \"txt\":\n",
    "        file_name = sub(\"-\",\"\",start_date) + '-' + sub(\"-\",\"\", end_date) + '-' + str(page_no) + '.txt'\n",
    "        file_ = open(DATA_PATH + file_name,\"w\") \n",
    "        file_.writelines(str(page_issues)) \n",
    "        file_.close() #to change file access modes \n",
    "        return \n",
    "\n",
    "    file_name = sub(\"-\",\"\",start_date) + '-' + sub(\"-\",\"\", end_date) + '-' + str(page_no) + '.pickle'\n",
    "    with open(DATA_PATH+ file_name, 'wb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        pickle.dump(page_issues, filehandle)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update page no of specified url\n",
    "def update_url_page_no(URL, page_no):\n",
    "    return sub(r'&p=[\\d]+', '&p='+str(page_no), URL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_CUR_PAGE_NO = 1\n",
    "GLOBAL_START_DATE = start_date\n",
    "GLOBAL_END_DATE = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(start_date, end_date, GLOBAL_CUR_PAGE_NO=1):\n",
    "    \n",
    "    data = []\n",
    "    while start_date < end_date: # running this loop to get all issuses from main start and end dates\n",
    "        print(\" ------------------------------------------------------------------------------------ \")\n",
    "        n_issues, pages_to_load = load_github_issues(start_date=start_date, end_date=end_date)\n",
    "        print(\"- Found %d issues and %d pages to load from %s and %s\"%(n_issues, pages_to_load, start_date, end_date))\n",
    "        approximated_end_date = end_date\n",
    "        \n",
    "        \n",
    "        while n_issues > 1000: # running this loop to approximate scrappable end date\n",
    "            approximated_end_date = decrease_end_date(start_date=start_date, \n",
    "                                                      end_date=approximated_end_date, \n",
    "                                                      smart_decrease=True, \n",
    "                                                      cur_result_cnt=n_issues, \n",
    "                                                      max_result_cnt=1000)\n",
    "            \n",
    "            n_issues, pages_to_load = load_github_issues(start_date=start_date, end_date=approximated_end_date)\n",
    "            print(\"- Found %d issues and %d pages to load from %s and %s\"%(n_issues, pages_to_load, start_date, approximated_end_date))\n",
    "            random_sleep()\n",
    "         \n",
    "        # scrapping over pages \n",
    "        cur_page_no = 1\n",
    "        \n",
    "        \n",
    "        if GLOBAL_CUR_PAGE_NO != 1: \n",
    "            cur_page_no = GLOBAL_CUR_PAGE_NO\n",
    "            GLOBAL_CUR_PAGE_NO = 1\n",
    "            \n",
    "            print(\"- Setting cur_page_no to %d -\"%cur_page_no)\n",
    "            \n",
    "            \n",
    "        cur_page_url = driver.current_url\n",
    "        while cur_page_no <= pages_to_load:\n",
    "            print(\"- Getting issues from page %d -\"%cur_page_no)\n",
    "            \n",
    "            # loading next page\n",
    "            if cur_page_no != 1:\n",
    "                cur_page_url = update_url_page_no(URL=cur_page_url, page_no=cur_page_no)\n",
    "                load_page_url(driver=driver, url=cur_page_url)\n",
    "            \n",
    "            # extracting issues\n",
    "            pg_source = get_page_source(driver)\n",
    "            page_issues = extract_page_issues(pg_source)\n",
    "            # saving output of every page\n",
    "            if (page_issues is not None and len(page_issues) > 0):\n",
    "                save_page_issues(page_issues=page_issues,\n",
    "                                 start_date=start_date, \n",
    "                                 end_date=approximated_end_date, \n",
    "                                 page_no=cur_page_no, \n",
    "                                 type_='pickle')\n",
    "            cur_page_no += 1 # updating page number\n",
    "        \n",
    "        # updating start date\n",
    "        start_date = approximated_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      " ------------------------------------------------------------------------------------ \n",
      "- Loading page using driver -\n",
      "- Sleeping for 14 seconds -\n",
      "- Getting Page Source -\n",
      "- Found 53 issues and 6 pages to load from 2012-01-01 and 2020-07-01\n",
      "- Getting issues from page 1 -\n",
      "- Getting Page Source -\n",
      "- Writing 10 issues from page_no: 1 between dates 2012-01-01 and 2020-07-01 -\n",
      "- Getting issues from page 2 -\n",
      "- Loading page using driver -\n",
      "- Sleeping for 10 seconds -\n",
      "- Getting Page Source -\n",
      "- Writing 10 issues from page_no: 2 between dates 2012-01-01 and 2020-07-01 -\n",
      "- Getting issues from page 3 -\n",
      "- Loading page using driver -\n",
      "- Sleeping for 14 seconds -\n",
      "- Getting Page Source -\n",
      "- Writing 10 issues from page_no: 3 between dates 2012-01-01 and 2020-07-01 -\n",
      "- Getting issues from page 4 -\n",
      "- Loading page using driver -\n",
      "- Sleeping for 12 seconds -\n",
      "- Getting Page Source -\n",
      "- Writing 10 issues from page_no: 4 between dates 2012-01-01 and 2020-07-01 -\n",
      "- Getting issues from page 5 -\n",
      "- Loading page using driver -\n",
      "- Sleeping for 13 seconds -\n",
      "- Getting Page Source -\n",
      "- Writing 10 issues from page_no: 5 between dates 2012-01-01 and 2020-07-01 -\n",
      "- Getting issues from page 6 -\n",
      "- Loading page using driver -\n",
      "- Sleeping for 14 seconds -\n",
      "- Getting Page Source -\n",
      "- Writing 10 issues from page_no: 6 between dates 2012-01-01 and 2020-07-01 -\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_START_DATE != start_date:\n",
    "    print(\"1\")\n",
    "    main(start_date=GLOBAL_START_DATE, end_date=GLOBAL_END_DATE, GLOBAL_CUR_PAGE_NO=GLOBAL_CUR_PAGE_NO)\n",
    "else:\n",
    "    print(\"2\")\n",
    "    main(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
